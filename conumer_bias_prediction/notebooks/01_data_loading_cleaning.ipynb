{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1e9e6a",
   "metadata": {},
   "source": [
    "# NOTEBOOK 1: DATA LOADING & CLEANING VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6408a508",
   "metadata": {},
   "source": [
    "# Data Loading and Cleaning\n",
    "\n",
    "**Purpose:** Load, standardize, and validate the MBIC Media Bias Annotation Dataset\n",
    "\n",
    "**Dataset Files:**\n",
    "- `annotators.csv` - Annotator demographic information\n",
    "- `annotations.xlsx` - Bias judgments by annotators  \n",
    "- `labeled_dataset.xlsx` - News articles with text content\n",
    "\n",
    "**Outputs:**\n",
    "- Standardized UTF-8 CSV files in `data/processed/`\n",
    "- Ready for exploratory data analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef89171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d11bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean special characters in dataframes\n",
    "def clean_special_characters(df):\n",
    "    \"\"\"Clean special characters that cause issues in all string columns\"\"\"\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[col] = df[col].astype(str).str.replace(\"’\", \"'\", regex=False)\n",
    "        df[col] = df[col].str.replace(\"“\", '\"', regex=False)\n",
    "        df[col] = df[col].str.replace(\"”\", '\"', regex=False)\n",
    "        df[col] = df[col].str.replace(\"–\", \"-\", regex=False)\n",
    "        df[col] = df[col].str.replace(\"—\", \"-\", regex=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fade0011",
   "metadata": {},
   "source": [
    "## Step 1: Detect File Encoding\n",
    "\n",
    "The `annotators.csv` file may have encoding issues. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26733419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File encoding: Windows-1252\n",
      "Confidence: 73.0%\n"
     ]
    }
   ],
   "source": [
    "# Create a function to detect the encoding of a CSV file\n",
    "# This function reads the file in binary mode and uses chardet to detect the encoding\n",
    "# It returns the encoding and confidence level of the detection\n",
    "def detect_encoding(file_path):\n",
    "    \"\"\"Detect the encoding of a CSV file using chardet\"\"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        raw_data = file.read()\n",
    "        result = chardet.detect(raw_data)\n",
    "        return result\n",
    "\n",
    "\n",
    "# Detect encoding of annotators.csv\n",
    "encoding_info = detect_encoding(\"../data/raw/annotators.csv\")\n",
    "print(f\"File encoding: {encoding_info['encoding']}\")\n",
    "print(f\"Confidence: {encoding_info['confidence']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df2f4f5",
   "metadata": {},
   "source": [
    "## Step 2: Standardize CSV File\n",
    "\n",
    "Convert the annotators.csv file to UTF-8 encoding with comma separators while protecting comma-containing cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89df0c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing: ../data/raw/annotators.csv\n",
      "Loaded shape: (1345, 9)\n",
      "Columns: ['id', 'age', 'gender', 'education', 'native_english_speaker', 'political_ideology', 'followed_news_outlets', 'news_check_frequency', 'survey_completed']\n",
      "Total missing values: 0\n",
      "Standardized file saved: ../data/processed/annotators_utf8.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to standardize CSV files\n",
    "# This function reads a CSV file with specified separator and encoding,\n",
    "# performs basic validation, and saves it with UTF-8 encoding and comma separator\n",
    "# It also prints the shape of the DataFrame and the list of columns\n",
    "# Additionally, it checks for missing data and prints the total count of missing values\n",
    "def safe_standardize_csv(\n",
    "    input_file, output_file, input_sep=\";\", input_encoding=\"windows-1252\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Standardize CSV file by converting to UTF-8 encoding and comma separators\n",
    "    \"\"\"\n",
    "    print(f\"Standardizing: {input_file}\")\n",
    "\n",
    "    # Load the CSV with detected parameters\n",
    "    df = pd.read_csv(input_file, sep=input_sep, encoding=input_encoding)\n",
    "\n",
    "    # Clean special characters\n",
    "    df = clean_special_characters(df)\n",
    "\n",
    "    # Basic validation\n",
    "    print(f\"Loaded shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Check for missing data\n",
    "    missing_data = df.isnull().sum().sum()\n",
    "    print(f\"Total missing values: {missing_data:,}\")\n",
    "\n",
    "    # Save with proper UTF-8 encoding\n",
    "    df.to_csv(output_file, encoding=\"utf-8\", index=False)\n",
    "    print(f\"Standardized file saved: {output_file}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Standardize the annotators.csv file\n",
    "annotators_standardized = safe_standardize_csv(\n",
    "    \"../data/raw/annotators.csv\",\n",
    "    \"../data/processed/annotators_utf8.csv\",\n",
    "    input_sep=\";\",\n",
    "    input_encoding=\"Windows-1252\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67cf1a",
   "metadata": {},
   "source": [
    "## Step 3: Convert Excel Files to CSV\n",
    "\n",
    "Load the Excel files and convert them to UTF-8 CSV format for consistency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfcfb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations loaded: (17775, 23)\n",
      "Labeled dataset loaded: (1700, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/issrar/miniconda3/envs/erdos_summer_2025/lib/python3.12/site-packages/openpyxl/worksheet/_read_only.py:85: UserWarning: Unknown extension is not supported and will be removed\n",
      "  for idx, row in parser.parse():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotations.xlsx -> annotations_utf8.csv\n",
      "labeled_dataset.xlsx -> labeled_dataset_utf8.csv\n"
     ]
    }
   ],
   "source": [
    "# Load Excel files\n",
    "annotations = pd.read_excel(\"../data/raw/annotations.xlsx\")\n",
    "annotations = clean_special_characters(annotations)\n",
    "print(f\"Annotations loaded: {annotations.shape}\")\n",
    "\n",
    "labeled_dataset = pd.read_excel(\"../data/raw/labeled_dataset.xlsx\")\n",
    "labeled_dataset = clean_special_characters(labeled_dataset)\n",
    "print(f\"Labeled dataset loaded: {labeled_dataset.shape}\")\n",
    "\n",
    "# Convert to UTF-8 CSV files\n",
    "annotations.to_csv(\n",
    "    \"../data/processed/annotations_utf8.csv\", encoding=\"utf-8\", index=False\n",
    ")\n",
    "print(\"annotations.xlsx -> annotations_utf8.csv\")\n",
    "\n",
    "labeled_dataset.to_csv(\n",
    "    \"../data/processed/labeled_dataset_utf8.csv\", encoding=\"utf-8\", index=False\n",
    ")\n",
    "print(\"labeled_dataset.xlsx -> labeled_dataset_utf8.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4f14e",
   "metadata": {},
   "source": [
    "## Step 4: Check Data Structure\n",
    "\n",
    "Examine the structure of each dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3b1e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotators (standardized): (1345, 9)\n",
      "Annotations: (17775, 23)\n",
      "Labeled dataset: (1700, 12)\n",
      "\n",
      "Dataset Structures:\n",
      "\n",
      "ANNOTATORS DATASET:\n",
      "Shape: 1,345 rows x 9 columns\n",
      "Columns: ['id', 'age', 'gender', 'education', 'native_english_speaker', 'political_ideology', 'followed_news_outlets', 'news_check_frequency', 'survey_completed']\n",
      "Data types:\n",
      "  id: object\n",
      "  age: int64\n",
      "  gender: object\n",
      "  education: object\n",
      "  native_english_speaker: object\n",
      "  political_ideology: int64\n",
      "  followed_news_outlets: object\n",
      "  news_check_frequency: object\n",
      "  survey_completed: object\n",
      "\n",
      "ANNOTATIONS DATASET:\n",
      "Shape: 17,775 rows x 23 columns\n",
      "Columns: ['Unnamed: 0.1', 'Unnamed: 0', 'survey_record_id', 'sentence_id', 'sentence_group_id', 'created_at', 'label', 'words', 'factual', 'group_id', 'text', 'link', 'type', 'topic', 'outlet', 'age', 'gender', 'education', 'native_english_speaker', 'political_ideology', 'followed_news_outlets', 'news_check_frequency', 'survey_completed']\n",
      "Data types:\n",
      "  Unnamed: 0.1: int64\n",
      "  Unnamed: 0: int64\n",
      "  survey_record_id: object\n",
      "  sentence_id: object\n",
      "  sentence_group_id: int64\n",
      "  created_at: object\n",
      "  label: object\n",
      "  words: object\n",
      "  factual: object\n",
      "  group_id: int64\n",
      "  text: object\n",
      "  link: object\n",
      "  type: object\n",
      "  topic: object\n",
      "  outlet: object\n",
      "  age: int64\n",
      "  gender: object\n",
      "  education: object\n",
      "  native_english_speaker: object\n",
      "  political_ideology: int64\n",
      "  followed_news_outlets: object\n",
      "  news_check_frequency: object\n",
      "  survey_completed: bool\n",
      "\n",
      "LABELED_DATASET DATASET:\n",
      "Shape: 1,700 rows x 12 columns\n",
      "Columns: ['Unnamed: 0', 'sentence', 'news_link', 'outlet', 'topic', 'type', 'group_id', 'num_sent', 'Label_bias', 'Label_opinion', 'article', 'biased_words4']\n",
      "Data types:\n",
      "  Unnamed: 0: int64\n",
      "  sentence: object\n",
      "  news_link: object\n",
      "  outlet: object\n",
      "  topic: object\n",
      "  type: object\n",
      "  group_id: int64\n",
      "  num_sent: int64\n",
      "  Label_bias: object\n",
      "  Label_opinion: object\n",
      "  article: object\n",
      "  biased_words4: object\n"
     ]
    }
   ],
   "source": [
    "# Check that everything loaded correctly\n",
    "print(f\"Annotators (standardized): {annotators_standardized.shape}\")\n",
    "print(f\"Annotations: {annotations.shape}\")\n",
    "print(f\"Labeled dataset: {labeled_dataset.shape}\")\n",
    "\n",
    "# Store datasets in dictionary\n",
    "datasets = {\n",
    "    \"annotators\": annotators_standardized,\n",
    "    \"annotations\": annotations,\n",
    "    \"labeled_dataset\": labeled_dataset,\n",
    "}\n",
    "\n",
    "# Analyze each dataset structure\n",
    "# This will print the shape, columns, and data types of each dataset\n",
    "print(\"\\nDataset Structures:\")\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n{name.upper()} DATASET:\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "    # Show data types\n",
    "    print(\"Data types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62e220",
   "metadata": {},
   "source": [
    "## Step 5: Check for Missing Values\n",
    "\n",
    "Identify any missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40b4ad02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MISSING VALUES - ANNOTATORS:\n",
      "No missing values detected\n",
      "\n",
      "MISSING VALUES - ANNOTATIONS:\n",
      "No missing values detected\n",
      "\n",
      "MISSING VALUES - LABELED_DATASET:\n",
      "No missing values detected\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in each dataset\n",
    "# This will print the count and percentage of missing values for each column\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nMISSING VALUES - {name.upper()}:\")\n",
    "\n",
    "    missing_summary = df.isnull().sum()\n",
    "\n",
    "    if missing_summary.sum() > 0:\n",
    "        print(\"Columns with missing values:\")\n",
    "        for col, missing_count in missing_summary[missing_summary > 0].items():\n",
    "            missing_pct = (missing_count / len(df)) * 100\n",
    "            print(f\"  {col}: {missing_count:,} ({missing_pct:.1f}%)\")\n",
    "    else:\n",
    "        print(\"No missing values detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ecada",
   "metadata": {},
   "source": [
    "## Step 6: Basic Data Cleaning\n",
    "\n",
    "Not analyzing patterns yet to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74aa244c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['Unnamed: 0.1', 'Unnamed: 0']\n",
      "Dropped survey_completed column\n",
      "Clean dataset shape: (17775, 20)\n"
     ]
    }
   ],
   "source": [
    "# Annotations dataset has both judgments and demographics\n",
    "## Remove unnecessary columns from annotations dataset\n",
    "\n",
    "# This will drop any unnamed columns and the survey completion column if it exists\n",
    "clean_annotations = annotations.copy()\n",
    "\n",
    "# Drop unnamed columns (usually index columns from Excel)\n",
    "cols_to_drop = [col for col in clean_annotations.columns if \"Unnamed\" in col]\n",
    "if cols_to_drop:\n",
    "    clean_annotations = clean_annotations.drop(columns=cols_to_drop)\n",
    "    print(f\"Dropped columns: {cols_to_drop}\")\n",
    "\n",
    "# Remove survey completion column (redundant because all are completed)\n",
    "if \"survey_completed\" in clean_annotations.columns:\n",
    "    clean_annotations = clean_annotations.drop(columns=[\"survey_completed\"])\n",
    "    print(\"Dropped survey_completed column\")\n",
    "\n",
    "print(f\"Clean dataset shape: {clean_annotations.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e937726f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fc2eb79",
   "metadata": {},
   "source": [
    "#### Ordinal encoding: Define ordinal mappings for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b052df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education: Created ordinal encoding\n"
     ]
    }
   ],
   "source": [
    "# Education\n",
    "education_mapping = {\n",
    "    \"Some high school\": 1,\n",
    "    \"High school graduate\": 2,\n",
    "    \"Vocational or technical school\": 3,\n",
    "    \"Some college\": 4,\n",
    "    \"Associate degree\": 5,\n",
    "    \"Bachelor's degree\": 6,\n",
    "    \"Graduate work\": 7,\n",
    "    \"I prefer not to say\": np.nan, # will handle below\n",
    "}\n",
    "# Apply education mapping\n",
    "if \"education\" in clean_annotations.columns:\n",
    "    clean_annotations[\"education_ordinal\"] = clean_annotations[\"education\"].map(\n",
    "        education_mapping\n",
    "    )\n",
    "    print(f\"Education: Created ordinal encoding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe0d1989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News frequency: Created ordinal encoding\n"
     ]
    }
   ],
   "source": [
    "# News checks frequency\n",
    "news_checks_mapping = {\n",
    "    \"Never\": 1,\n",
    "    \"Very rarely\": 2,\n",
    "    \"Several times per month\": 3,\n",
    "    \"Several times per week\": 4,\n",
    "    \"Every day\": 5,\n",
    "    \"Several times per day\": 6,\n",
    "}\n",
    "\n",
    "# Apply news checks frequency mapping\n",
    "if \"news_check_frequency\" in clean_annotations.columns:\n",
    "    clean_annotations[\"news_frequency_ordinal\"] = clean_annotations[\n",
    "        \"news_check_frequency\"\n",
    "    ].map(news_checks_mapping)\n",
    "    print(f\"News frequency: Created ordinal encoding\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1b799d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English proficiency: Created ordinal encoding\n"
     ]
    }
   ],
   "source": [
    "# Native English speaker\n",
    "native_english_mapping = {\n",
    "    \"Non-native speaker\": 1,\n",
    "    \"Near-native speaker\": 2,\n",
    "    \"Native speaker\": 3,\n",
    "}\n",
    "\n",
    "# Apply native English speaker mapping\n",
    "if \"native_english_speaker\" in clean_annotations.columns:\n",
    "    clean_annotations[\"english_ordinal\"] = clean_annotations[\n",
    "        \"native_english_speaker\"\n",
    "    ].map(native_english_mapping)\n",
    "    print(f\"English proficiency: Created ordinal encoding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62b9966",
   "metadata": {},
   "source": [
    "- Create `is_biased` variable based on `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82ea06df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Not Biased (0): 7,124 (40.1%)\n",
      "  Biased (1): 10,651 (59.9%)\n"
     ]
    }
   ],
   "source": [
    "clean_annotations[\"is_biased\"] = (clean_annotations[\"label\"] == \"Biased\").astype(int)\n",
    "\n",
    "# Verify `label` conversion\n",
    "bias_counts = clean_annotations[\"is_biased\"].value_counts()\n",
    "total = len(clean_annotations)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"  Not Biased (0): {bias_counts.get(0, 0):,} ({bias_counts.get(0, 0) / total:.1%})\"\n",
    ")\n",
    "print(f\"  Biased (1): {bias_counts.get(1, 0):,} ({bias_counts.get(1, 0) / total:.1%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712ca701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values\n"
     ]
    }
   ],
   "source": [
    "# Check for any issues in `label` conversion\n",
    "target_missing = clean_annotations[\"is_biased\"].isnull().sum()\n",
    "if target_missing > 0:\n",
    "    print(f\"Warning: {target_missing} missing values in converted labels\")\n",
    "else:\n",
    "    print(\"No missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6023cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New ordinal features created:\n",
      "  education_ordinal: 7 unique values\n",
      "  news_frequency_ordinal: 6 unique values\n",
      "  english_ordinal: 3 unique values\n"
     ]
    }
   ],
   "source": [
    "# Check the new features\n",
    "print(f\"\\nNew ordinal features created:\")\n",
    "ordinal_cols = [\"education_ordinal\", \"news_frequency_ordinal\", \"english_ordinal\"]\n",
    "for col in ordinal_cols:\n",
    "    if col in clean_annotations.columns:\n",
    "        print(f\"  {col}: {clean_annotations[col].nunique()} unique values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1105dcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age: Replaced 20 zero values with median (35)\n",
      "Age range: 18 to 71\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Age: resolve age = 0 issues -> fill with median\n",
    "if \"age\" in clean_annotations.columns:\n",
    "    # Check for age = 0 (invalid ages)\n",
    "    age_zero_count = (clean_annotations[\"age\"] == 0).sum()\n",
    "\n",
    "    if age_zero_count > 0:\n",
    "        # Calculate median from valid ages (excluding 0)\n",
    "        valid_ages = clean_annotations[\"age\"][clean_annotations[\"age\"] > 0]\n",
    "        median_age = valid_ages.median()\n",
    "\n",
    "        # Replace age = 0 with median\n",
    "        clean_annotations.loc[clean_annotations[\"age\"] == 0, \"age\"] = median_age\n",
    "\n",
    "        print(\n",
    "            f\"Age: Replaced {age_zero_count} zero values with median ({median_age:.0f})\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Age: No zero values found\")\n",
    "\n",
    "    # Show age distribution after fix\n",
    "    print(\n",
    "        f\"Age range: {clean_annotations['age'].min():.0f} to {clean_annotations['age'].max():.0f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3040201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Education: Created ordinal encoding, filled 60 missing with median (6)\n",
      "News frequency: Created ordinal encoding, filled 0 missing with median (5)\n",
      "English proficiency: Created ordinal encoding, filled 0 missing with median (3)\n"
     ]
    }
   ],
   "source": [
    "# Apply ordinal mappings\n",
    "\n",
    "# Education \n",
    "\n",
    "if \"education\" in clean_annotations.columns:\n",
    "    clean_annotations[\"education_ordinal\"] = clean_annotations[\"education\"].map(\n",
    "        education_mapping\n",
    "    )\n",
    "    # Count missing before imputation\n",
    "    missing_count = clean_annotations[\"education_ordinal\"].isnull().sum()\n",
    "    median_education = clean_annotations[\"education_ordinal\"].median()\n",
    "    clean_annotations[\"education_ordinal\"] = clean_annotations[\n",
    "        \"education_ordinal\"\n",
    "    ].fillna(median_education)\n",
    "    print(\n",
    "        f\"Education: Created ordinal encoding, filled {missing_count} missing with median ({median_education:.0f})\"\n",
    "    )\n",
    "\n",
    "# News checks frequency\n",
    "if \"news_check_frequency\" in clean_annotations.columns:\n",
    "    clean_annotations[\"news_frequency_ordinal\"] = clean_annotations[\n",
    "        \"news_check_frequency\"\n",
    "    ].map(news_checks_mapping)\n",
    "    missing_count = clean_annotations[\"news_frequency_ordinal\"].isnull().sum()\n",
    "    median_news = clean_annotations[\"news_frequency_ordinal\"].median()\n",
    "    clean_annotations[\"news_frequency_ordinal\"] = clean_annotations[\n",
    "        \"news_frequency_ordinal\"\n",
    "    ].fillna(median_news)\n",
    "    print(\n",
    "        f\"News frequency: Created ordinal encoding, filled {missing_count} missing with median ({median_news:.0f})\"\n",
    "    )\n",
    "\n",
    "# Native English speaker\n",
    "if \"native_english_speaker\" in clean_annotations.columns:\n",
    "    clean_annotations[\"english_ordinal\"] = clean_annotations[\n",
    "        \"native_english_speaker\"\n",
    "    ].map(native_english_mapping)\n",
    "    missing_count = clean_annotations[\"english_ordinal\"].isnull().sum()\n",
    "    median_english = clean_annotations[\"english_ordinal\"].median()\n",
    "    clean_annotations[\"english_ordinal\"] = clean_annotations[\"english_ordinal\"].fillna(\n",
    "        median_english\n",
    "    )\n",
    "    print(\n",
    "        f\"English proficiency: Created ordinal encoding, filled {missing_count} missing with median ({median_english:.0f})\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97a10e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Final check for missing values\n",
    "remaining_missing = clean_annotations.isnull().sum().sum()\n",
    "print(f\"\\nRemaining missing values: {remaining_missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cb0b14",
   "metadata": {},
   "source": [
    "# Feature inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42f40fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (17775, 24)\n",
      "\n",
      "Actual columns available:\n",
      "   1. survey_record_id\n",
      "   2. sentence_id\n",
      "   3. sentence_group_id\n",
      "   4. created_at\n",
      "   5. label\n",
      "   6. words\n",
      "   7. factual\n",
      "   8. group_id\n",
      "   9. text\n",
      "  10. link\n",
      "  11. type\n",
      "  12. topic\n",
      "  13. outlet\n",
      "  14. age\n",
      "  15. gender\n",
      "  16. education\n",
      "  17. native_english_speaker\n",
      "  18. political_ideology\n",
      "  19. followed_news_outlets\n",
      "  20. news_check_frequency\n",
      "  21. education_ordinal\n",
      "  22. news_frequency_ordinal\n",
      "  23. english_ordinal\n",
      "  24. is_biased\n",
      "\n",
      "Data types and completeness:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17775 entries, 0 to 17774\n",
      "Data columns (total 24 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   survey_record_id        17775 non-null  object \n",
      " 1   sentence_id             17775 non-null  object \n",
      " 2   sentence_group_id       17775 non-null  int64  \n",
      " 3   created_at              17775 non-null  object \n",
      " 4   label                   17775 non-null  object \n",
      " 5   words                   17775 non-null  object \n",
      " 6   factual                 17775 non-null  object \n",
      " 7   group_id                17775 non-null  int64  \n",
      " 8   text                    17775 non-null  object \n",
      " 9   link                    17775 non-null  object \n",
      " 10  type                    17775 non-null  object \n",
      " 11  topic                   17775 non-null  object \n",
      " 12  outlet                  17775 non-null  object \n",
      " 13  age                     17775 non-null  int64  \n",
      " 14  gender                  17775 non-null  object \n",
      " 15  education               17775 non-null  object \n",
      " 16  native_english_speaker  17775 non-null  object \n",
      " 17  political_ideology      17775 non-null  int64  \n",
      " 18  followed_news_outlets   17775 non-null  object \n",
      " 19  news_check_frequency    17775 non-null  object \n",
      " 20  education_ordinal       17775 non-null  float64\n",
      " 21  news_frequency_ordinal  17775 non-null  int64  \n",
      " 22  english_ordinal         17775 non-null  int64  \n",
      " 23  is_biased               17775 non-null  int64  \n",
      "dtypes: float64(1), int64(7), object(16)\n",
      "memory usage: 3.3+ MB\n",
      "None\n",
      "\n",
      "Sample of actual data:\n",
      "                   survey_record_id                       sentence_id  \\\n",
      "0  0045473f40ec42a2bd2ca0ee35df0b75  06e9e57e549d4dd48d8ac649ff81fd2e   \n",
      "1  0045473f40ec42a2bd2ca0ee35df0b75  07f2137fd3ae4dd2b5c990b93e5c2a62   \n",
      "2  0045473f40ec42a2bd2ca0ee35df0b75  10a5e68a84ab4c1a83d861b87c57def9   \n",
      "\n",
      "   sentence_group_id           created_at       label words  \\\n",
      "0                 67  2020-08-12 06:09:53  Non-biased   nan   \n",
      "1                 67  2020-08-12 06:10:21  Non-biased   nan   \n",
      "2                 67  2020-08-12 06:10:35  Non-biased   nan   \n",
      "\n",
      "                                 factual  group_id  \\\n",
      "0                       Entirely factual        67   \n",
      "1             Expresses writer's opinion        67   \n",
      "2  Somewhat factual but also opinionated        67   \n",
      "\n",
      "                                                text  \\\n",
      "0  The transgender effort to suppress any recogni...   \n",
      "1  Radical Virginia Citizens Defense League has o...   \n",
      "2  Miller is the architect of President Donald Tr...   \n",
      "\n",
      "                                                link  ... gender  \\\n",
      "0  https://www.breitbart.com/politics/2019/02/21/...  ...   Male   \n",
      "1  https://www.alternet.org/2020/01/pro-gun-prote...  ...   Male   \n",
      "2  https://www.nbcnews.com/news/latino/after-step...  ...   Male   \n",
      "\n",
      "           education native_english_speaker  political_ideology  \\\n",
      "0  Bachelor's degree         Native speaker                   7   \n",
      "1  Bachelor's degree         Native speaker                   7   \n",
      "2  Bachelor's degree         Native speaker                   7   \n",
      "\n",
      "   followed_news_outlets news_check_frequency education_ordinal  \\\n",
      "0  ['ABC News', 'MSNBC']            Every day               6.0   \n",
      "1  ['ABC News', 'MSNBC']            Every day               6.0   \n",
      "2  ['ABC News', 'MSNBC']            Every day               6.0   \n",
      "\n",
      "   news_frequency_ordinal english_ordinal is_biased  \n",
      "0                       5               3         0  \n",
      "1                       5               3         0  \n",
      "2                       5               3         0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Dataset shape:\", clean_annotations.shape)\n",
    "print(\"\\nActual columns available:\")\n",
    "for i, col in enumerate(clean_annotations.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Check data types and non-null counts\n",
    "print(f\"\\nData types and completeness:\")\n",
    "print(clean_annotations.info())\n",
    "\n",
    "# Sample of actual data\n",
    "print(f\"\\nSample of actual data:\")\n",
    "print(clean_annotations.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0f6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned annotations saved to: ../data/processed/clean_annotations.csv\n"
     ]
    }
   ],
   "source": [
    "# Safe the cleaned annotations dataset\n",
    "clean_annotations.to_csv(\n",
    "    \"../data/processed/clean_annotations.csv\", encoding=\"utf-8\", index=False\n",
    ")\n",
    "print(\"Cleaned annotations saved to: ../data/processed/clean_annotations.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1291855",
   "metadata": {},
   "source": [
    "# Final check before moving to next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd599c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEWS OUTLET DATA EXPLORATION\n",
      "============================\n",
      "1. FOLLOWED NEWS OUTLETS COLUMN:\n",
      "   Sample values:\n",
      "[\"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\", \"['ABC News', 'MSNBC']\"]\n",
      "\n",
      "   Data type: object\n",
      "   Unique values: 551\n",
      "   Missing values: 0\n",
      "\n",
      "2. UNIQUE FOLLOWED NEWS OUTLETS:\n",
      "   Total unique combinations: 551\n",
      "\n",
      "   Top 20 most common combinations:\n",
      "followed_news_outlets\n",
      "['CNN']                                      1100\n",
      "['Fox News']                                  980\n",
      "['New York Times']                            679\n",
      "['The Washington Post']                       320\n",
      "['USA Today']                                 300\n",
      "['ABC News']                                  280\n",
      "['MSNBC']                                     240\n",
      "['Fox News', 'CNN']                           220\n",
      "['New York Times', 'The Washington Post']     180\n",
      "['Fox News', 'Breitbart']                     160\n",
      "['New York Times', 'CNN']                     160\n",
      "['Fox News', 'New York Times']                140\n",
      "['The Guardian']                              140\n",
      "['CNN', 'MSNBC']                              120\n",
      "['MSNBC', 'CNN']                              120\n",
      "['CBS News']                                  120\n",
      "['NPR']                                       120\n",
      "['CNN', 'Fox News']                           100\n",
      "['CNN', 'The Washington Post']                100\n",
      "['Fox News', 'USA Today']                     100\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3. OUTLET FORMAT ANALYSIS:\n",
      "   Sample outlet entries:\n",
      "    1. ['ABC News', 'MSNBC']\n",
      "    2. ['ABC News', 'MSNBC']\n",
      "    3. ['ABC News', 'MSNBC']\n",
      "    4. ['ABC News', 'MSNBC']\n",
      "    5. ['ABC News', 'MSNBC']\n",
      "    6. ['ABC News', 'MSNBC']\n",
      "    7. ['ABC News', 'MSNBC']\n",
      "    8. ['ABC News', 'MSNBC']\n",
      "    9. ['ABC News', 'MSNBC']\n",
      "   10. ['ABC News', 'MSNBC']\n",
      "   11. ['ABC News', 'MSNBC']\n",
      "   12. ['ABC News', 'MSNBC']\n",
      "   13. ['ABC News', 'MSNBC']\n",
      "   14. ['ABC News', 'MSNBC']\n",
      "   15. ['ABC News', 'MSNBC']\n",
      "   16. ['ABC News', 'MSNBC']\n",
      "   17. ['ABC News', 'MSNBC']\n",
      "   18. ['ABC News', 'MSNBC']\n",
      "   19. ['ABC News', 'MSNBC']\n",
      "   20. ['ABC News', 'MSNBC']\n",
      "\n",
      "4. SPECIFIC OUTLET ANALYSIS:\n",
      "   Total non-null entries: 17775\n",
      "   Entries with commas: 12376\n",
      "   Entries with semicolons: 0\n",
      "   Entries with pipes: 17775\n",
      "\n",
      "5. CURRENT STATE OF ORDINAL VARIABLES:\n",
      "   Education values:\n",
      "education\n",
      "Bachelor's degree                 8796\n",
      "Some college                      2680\n",
      "Graduate work                     2499\n",
      "Associate degree                  1780\n",
      "High school graduate              1340\n",
      "Vocational or technical school     460\n",
      "Some high school                   160\n",
      "I prefer not to say                 60\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   Education ordinal exists: True\n",
      "   Education ordinal values:\n",
      "education_ordinal\n",
      "1.0     160\n",
      "2.0    1340\n",
      "3.0     460\n",
      "4.0    2680\n",
      "5.0    1780\n",
      "6.0    8856\n",
      "7.0    2499\n",
      "Name: count, dtype: int64\n",
      "\n",
      "6. NEWS FREQUENCY VALUES:\n",
      "news_check_frequency\n",
      "Every day                  8013\n",
      "Several times per day      4180\n",
      "Several times per week     3402\n",
      "Several times per month    1180\n",
      "Very rarely                 820\n",
      "Never                       180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   News frequency ordinal exists: True\n",
      "   News frequency ordinal values:\n",
      "news_frequency_ordinal\n",
      "1     180\n",
      "2     820\n",
      "3    1180\n",
      "4    3402\n",
      "5    8013\n",
      "6    4180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "7. ENGLISH PROFICIENCY VALUES:\n",
      "native_english_speaker\n",
      "Native speaker         17315\n",
      "Near-native speaker      440\n",
      "Non-native speaker        20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "   English ordinal exists: True\n",
      "   English ordinal values:\n",
      "english_ordinal\n",
      "1       20\n",
      "2      440\n",
      "3    17315\n",
      "Name: count, dtype: int64\n",
      "\n",
      "8. OVERALL DATASET STATE:\n",
      "   Dataset shape: (17775, 24)\n",
      "   Columns: ['survey_record_id', 'sentence_id', 'sentence_group_id', 'created_at', 'label', 'words', 'factual', 'group_id', 'text', 'link', 'type', 'topic', 'outlet', 'age', 'gender', 'education', 'native_english_speaker', 'political_ideology', 'followed_news_outlets', 'news_check_frequency', 'education_ordinal', 'news_frequency_ordinal', 'english_ordinal', 'is_biased']\n",
      "\n",
      "   Missing values by column:\n",
      "\n",
      "9. REMAINING MISSING VALUES:\n",
      "   No remaining missing values detected\n"
     ]
    }
   ],
   "source": [
    "# Final checks before moving to next step\n",
    "# 1. Check the followed_news_outlets column\n",
    "print(\"1. FOLLOWED NEWS OUTLETS COLUMN:\")\n",
    "print(\"   Sample values:\")\n",
    "print(clean_annotations['followed_news_outlets'].head(10).tolist())\n",
    "print(f\"\\n   Data type: {clean_annotations['followed_news_outlets'].dtype}\")\n",
    "print(f\"   Unique values: {clean_annotations['followed_news_outlets'].nunique()}\")\n",
    "print(f\"   Missing values: {clean_annotations['followed_news_outlets'].isnull().sum()}\")\n",
    "# 2. Check unique values in followed_news_outlets\n",
    "print(\"\\n2. UNIQUE FOLLOWED NEWS OUTLETS:\")\n",
    "unique_outlets = clean_annotations['followed_news_outlets'].value_counts()\n",
    "print(f\"   Total unique combinations: {len(unique_outlets)}\")\n",
    "print(\"\\n   Top 20 most common combinations:\")\n",
    "print(unique_outlets.head(20))\n",
    "# 3. Check if outlets are comma-separated or different format\n",
    "print(\"\\n3. OUTLET FORMAT ANALYSIS:\")\n",
    "sample_outlets = clean_annotations['followed_news_outlets'].dropna().head(20)\n",
    "print(\"   Sample outlet entries:\")\n",
    "for i, outlet in enumerate(sample_outlets, 1):\n",
    "    print(f\"   {i:2d}. {outlet}\")\n",
    "# 4. Check for specific outlet names mentioned\n",
    "print(\"\\n4. SPECIFIC OUTLET ANALYSIS:\")\n",
    "outlet_sample = clean_annotations['followed_news_outlets'].dropna()\n",
    "print(f\"   Total non-null entries: {len(outlet_sample)}\")\n",
    "\n",
    "# Check for common separators\n",
    "has_comma = outlet_sample.str.contains(',', na=False).sum()\n",
    "has_semicolon = outlet_sample.str.contains(';', na=False).sum()\n",
    "has_pipe = outlet_sample.str.contains('|', na=False).sum()\n",
    "\n",
    "print(f\"   Entries with commas: {has_comma}\")\n",
    "print(f\"   Entries with semicolons: {has_semicolon}\")\n",
    "print(f\"   Entries with pipes: {has_pipe}\")\n",
    "# 5. Check current state of education and other ordinal variables\n",
    "print(\"\\n5. CURRENT STATE OF ORDINAL VARIABLES:\")\n",
    "print(\"   Education values:\")\n",
    "print(clean_annotations['education'].value_counts())\n",
    "print(f\"\\n   Education ordinal exists: {'education_ordinal' in clean_annotations.columns}\")\n",
    "if 'education_ordinal' in clean_annotations.columns:\n",
    "    print(\"   Education ordinal values:\")\n",
    "    print(clean_annotations['education_ordinal'].value_counts().sort_index())\n",
    "# 6. Check news frequency\n",
    "print(\"\\n6. NEWS FREQUENCY VALUES:\")\n",
    "print(clean_annotations['news_check_frequency'].value_counts())\n",
    "print(f\"\\n   News frequency ordinal exists: {'news_frequency_ordinal' in clean_annotations.columns}\")\n",
    "if 'news_frequency_ordinal' in clean_annotations.columns:\n",
    "    print(\"   News frequency ordinal values:\")\n",
    "    print(clean_annotations['news_frequency_ordinal'].value_counts().sort_index())\n",
    "# 7. Check English proficiency\n",
    "print(\"\\n7. ENGLISH PROFICIENCY VALUES:\")\n",
    "print(clean_annotations['native_english_speaker'].value_counts())\n",
    "print(f\"\\n   English ordinal exists: {'english_ordinal' in clean_annotations.columns}\")\n",
    "if 'english_ordinal' in clean_annotations.columns:\n",
    "    print(\"   English ordinal values:\")\n",
    "    print(clean_annotations['english_ordinal'].value_counts().sort_index())\n",
    "# 8. Check overall dataset state\n",
    "print(\"\\n8. OVERALL DATASET STATE:\")\n",
    "print(f\"   Dataset shape: {clean_annotations.shape}\")\n",
    "print(f\"   Columns: {list(clean_annotations.columns)}\")\n",
    "print(f\"\\n   Missing values by column:\")\n",
    "missing_summary = clean_annotations.isnull().sum()\n",
    "for col, missing in missing_summary[missing_summary > 0].items():\n",
    "    print(f\"   {col}: {missing}\")\n",
    "# 9. Check for any remaining missing values\n",
    "print(\"\\n9. REMAINING MISSING VALUES:\")\n",
    "remaining_missing = clean_annotations.isnull().sum().sum()\n",
    "if remaining_missing > 0:\n",
    "    print(f\"   Total remaining missing values: {remaining_missing}\")\n",
    "else:\n",
    "    print(\"   No remaining missing values detected\")        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284f5f15",
   "metadata": {},
   "source": [
    "- Step: Building news outlet features based on data format and content\n",
    "- PURPOSE: Create outlet categorization from string-formatted Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1cd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned annotations DataFrame\n",
    "clean_annotations = pd.read_csv(\"../data/processed/clean_annotations.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c58ed",
   "metadata": {},
   "source": [
    "# Step 1: Design news outlet features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edca1a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REDESIGNED NEWS OUTLET FEATURES\n",
      "===============================\n",
      "Applying outlet feature extraction...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nREDESIGNED NEWS OUTLET FEATURES\")\n",
    "print(\"=\" * 31)\n",
    "\n",
    "\n",
    "def extract_outlet_features(clean_annotations):\n",
    "    \"\"\"\n",
    "    Extract news outlet features from string-formatted Python lists\n",
    "    \"\"\"\n",
    "\n",
    "    # Create copy of the DataFrame to avoid modifying the original\n",
    "    df = clean_annotations.copy()\n",
    "\n",
    "    # Parse the string-formatted lists into actual lists\n",
    "    def parse_outlets(outlet_string):\n",
    "        try:\n",
    "            # Use ast.literal_eval to safely parse the string representation\n",
    "            return ast.literal_eval(outlet_string)\n",
    "        except:\n",
    "            # Fallback for any parsing issues\n",
    "            return []\n",
    "\n",
    "    df[\"outlets_list\"] = df[\"followed_news_outlets\"].apply(parse_outlets)\n",
    "\n",
    "    # Count total outlets followed\n",
    "    df[\"followed_outlets_count\"] = df[\"outlets_list\"].apply(len)\n",
    "\n",
    "    # Define outlet categories\n",
    "    mainstream_outlets = {\n",
    "        \"CNN\",\n",
    "        \"ABC News\",\n",
    "        \"CBS News\",\n",
    "        \"NBC News\",\n",
    "        \"USA Today\",\n",
    "        \"NPR\",\n",
    "        \"Reuters\",\n",
    "        \"Associated Press\",\n",
    "        \"BBC\",\n",
    "    }\n",
    "\n",
    "    conservative_outlets = {\n",
    "        \"Fox News\",\n",
    "        \"Breitbart\",\n",
    "        \"The Daily Wire\",\n",
    "        \"National Review\",\n",
    "        \"New York Post\",\n",
    "        \"The Wall Street Journal\",\n",
    "        \"Washington Examiner\",\n",
    "    }\n",
    "\n",
    "    liberal_outlets = {\n",
    "        \"MSNBC\",\n",
    "        \"The Guardian\",\n",
    "        \"Huffington Post\",\n",
    "        \"Vox\",\n",
    "        \"Slate\",\n",
    "        \"Mother Jones\",\n",
    "        \"The Nation\",\n",
    "    }\n",
    "\n",
    "    national_papers = {\n",
    "        \"New York Times\",\n",
    "        \"The Washington Post\",\n",
    "        \"The Wall Street Journal\",\n",
    "        \"The Guardian\",\n",
    "        \"NPR\",\n",
    "        \"BBC\",\n",
    "    }\n",
    "\n",
    "    # Create binary features for outlet categories\n",
    "    def check_outlet_category(outlets_list, category_set):\n",
    "        return int(any(outlet in category_set for outlet in outlets_list))\n",
    "\n",
    "    df[\"follows_mainstream\"] = df[\"outlets_list\"].apply(\n",
    "        lambda x: check_outlet_category(x, mainstream_outlets)\n",
    "    )\n",
    "\n",
    "    df[\"follows_conservative\"] = df[\"outlets_list\"].apply(\n",
    "        lambda x: check_outlet_category(x, conservative_outlets)\n",
    "    )\n",
    "\n",
    "    df[\"follows_liberal\"] = df[\"outlets_list\"].apply(\n",
    "        lambda x: check_outlet_category(x, liberal_outlets)\n",
    "    )\n",
    "\n",
    "    df[\"national_papers\"] = df[\"outlets_list\"].apply(\n",
    "        lambda x: check_outlet_category(x, national_papers)\n",
    "    )\n",
    "\n",
    "    # Additional features\n",
    "    df[\"follows_mixed_ideology\"] = (\n",
    "        (df[\"follows_conservative\"] == 1) & (df[\"follows_liberal\"] == 1)\n",
    "    ).astype(int)\n",
    "\n",
    "    df[\"follows_only_conservative\"] = (\n",
    "        (df[\"follows_conservative\"] == 1) & (df[\"follows_liberal\"] == 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    df[\"follows_only_liberal\"] = (\n",
    "        (df[\"follows_liberal\"] == 1) & (df[\"follows_conservative\"] == 0)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Specific high-frequency outlets\n",
    "    df[\"follows_cnn\"] = df[\"outlets_list\"].apply(lambda x: int(\"CNN\" in x))\n",
    "\n",
    "    df[\"follows_fox\"] = df[\"outlets_list\"].apply(lambda x: int(\"Fox News\" in x))\n",
    "\n",
    "    df[\"follows_nyt\"] = df[\"outlets_list\"].apply(lambda x: int(\"New York Times\" in x))\n",
    "\n",
    "    df[\"follows_wapo\"] = df[\"outlets_list\"].apply(\n",
    "        lambda x: int(\"The Washington Post\" in x)\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the feature extraction\n",
    "print(\"Applying outlet feature extraction...\")\n",
    "annotations_with_outlets = extract_outlet_features(clean_annotations)\n",
    "\n",
    "print(\"Outlet feature extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b8b7a",
   "metadata": {},
   "source": [
    "## Validation of new news outlet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19758172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION OF NEW OUTLET FEATURES\n",
      "=================================\n",
      "FEATURE VALIDATION:\n",
      "\n",
      "followed_outlets_count:\n",
      "   Values: {1: np.int64(5399), 2: np.int64(3513), 3: np.int64(2880), 4: np.int64(2163), 5: np.int64(1380), 6: np.int64(720), 7: np.int64(580), 8: np.int64(480), 9: np.int64(220), 10: np.int64(140), 11: np.int64(80), 12: np.int64(40), 13: np.int64(60), 14: np.int64(100), 15: np.int64(20)}\n",
      "   Range: 1 to 15\n",
      "   Mean: 3.15\n",
      "\n",
      "follows_mainstream:\n",
      "   Values: {0: np.int64(5772), 1: np.int64(12003)}\n",
      "\n",
      "follows_conservative:\n",
      "   Values: {0: np.int64(9772), 1: np.int64(8003)}\n",
      "\n",
      "follows_liberal:\n",
      "   Values: {0: np.int64(12662), 1: np.int64(5113)}\n",
      "\n",
      "follows_mixed_ideology:\n",
      "   Values: {0: np.int64(15255), 1: np.int64(2520)}\n",
      "\n",
      "follows_only_conservative:\n",
      "   Values: {0: np.int64(12292), 1: np.int64(5483)}\n",
      "\n",
      "follows_only_liberal:\n",
      "   Values: {0: np.int64(15182), 1: np.int64(2593)}\n",
      "\n",
      "follows_cnn:\n",
      "   Values: {0: np.int64(9892), 1: np.int64(7883)}\n",
      "\n",
      "follows_fox:\n",
      "   Values: {0: np.int64(12392), 1: np.int64(5383)}\n",
      "\n",
      "follows_nyt:\n",
      "   Values: {0: np.int64(11033), 1: np.int64(6742)}\n",
      "\n",
      "follows_wapo:\n",
      "   Values: {0: np.int64(13222), 1: np.int64(4553)}\n",
      "\n",
      "CROSS-VALIDATION WITH KNOWN PATTERNS:\n",
      "Checking against known outlet combinations...\n",
      "CNN solo followers: 1100 (should be ~1,100)\n",
      "   follows_cnn=1: 1100\n",
      "   follows_mainstream=1: 1100\n",
      "\n",
      "Fox News solo followers: 980 (should be ~980)\n",
      "   follows_fox=1: 980\n",
      "   follows_conservative=1: 980\n",
      "\n",
      "Fox + CNN followers: 320 (should be ~320)\n",
      "   follows_mixed_ideology=1: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the new features\n",
    "outlet_features = [\n",
    "    \"followed_outlets_count\",\n",
    "    \"follows_mainstream\",\n",
    "    \"follows_conservative\",\n",
    "    \"follows_liberal\",\n",
    "    \"follows_prestige\",\n",
    "    \"follows_mixed_ideology\",\n",
    "    \"follows_only_conservative\",\n",
    "    \"follows_only_liberal\",\n",
    "    \"follows_cnn\",\n",
    "    \"follows_fox\",\n",
    "    \"follows_nyt\",\n",
    "    \"follows_wapo\",\n",
    "]\n",
    "\n",
    "print(\"FEATURE VALIDATION:\")\n",
    "for feature in outlet_features:\n",
    "    if feature in annotations_with_outlets.columns:\n",
    "        values = annotations_with_outlets[feature].value_counts().sort_index()\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"   Values: {dict(values)}\")\n",
    "        if feature == \"followed_outlets_count\":\n",
    "            print(\n",
    "                f\"   Range: {annotations_with_outlets[feature].min()} to {annotations_with_outlets[feature].max()}\"\n",
    "            )\n",
    "            print(f\"   Mean: {annotations_with_outlets[feature].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef823455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation with known patterns\n",
    "print(f\"\\nCROSS-VALIDATION WITH KNOWN PATTERNS:\")\n",
    "print(\"Checking against known outlet combinations...\")\n",
    "\n",
    "# Check CNN solo followers\n",
    "cnn_solo = annotations_with_outlets[\n",
    "    annotations_with_outlets[\"followed_news_outlets\"] == \"['CNN']\"\n",
    "]\n",
    "print(f\"CNN solo followers: {len(cnn_solo)} (should be ~1,100)\")\n",
    "print(f\"   follows_cnn=1: {cnn_solo['follows_cnn'].sum()}\")\n",
    "print(f\"   follows_mainstream=1: {cnn_solo['follows_mainstream'].sum()}\")\n",
    "\n",
    "# Check Fox News solo followers\n",
    "fox_solo = annotations_with_outlets[\n",
    "    annotations_with_outlets[\"followed_news_outlets\"] == \"['Fox News']\"\n",
    "]\n",
    "print(f\"\\nFox News solo followers: {len(fox_solo)} (should be ~980)\")\n",
    "print(f\"   follows_fox=1: {fox_solo['follows_fox'].sum()}\")\n",
    "print(f\"   follows_conservative=1: {fox_solo['follows_conservative'].sum()}\")\n",
    "\n",
    "# Check mixed ideology followers\n",
    "fox_cnn = annotations_with_outlets[\n",
    "    annotations_with_outlets[\"followed_news_outlets\"].isin(\n",
    "        [\"['Fox News', 'CNN']\", \"['CNN', 'Fox News']\"]\n",
    "    )\n",
    "]\n",
    "print(f\"\\nFox + CNN followers: {len(fox_cnn)} (should be ~320)\")\n",
    "print(f\"   follows_mixed_ideology=1: {fox_cnn['follows_mixed_ideology'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382956e4",
   "metadata": {},
   "source": [
    "# STEP 2: CONSUMER-LEVEL AGGREGATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3eba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consumer-level dataset with all features\n",
    "consumer_data = (\n",
    "    annotations_with_outlets.groupby(\"survey_record_id\")\n",
    "    .agg(\n",
    "        {\n",
    "            # Target calculation components\n",
    "            \"is_biased\": [\"count\", \"sum\"],\n",
    "            # Demographics (pre-existing characteristics)\n",
    "            \"age\": \"first\",\n",
    "            \"gender\": \"first\",\n",
    "            \"education\": \"first\",\n",
    "            \"native_english_speaker\": \"first\",\n",
    "            \"political_ideology\": \"first\",\n",
    "            \"news_check_frequency\": \"first\",\n",
    "            # Ordinal versions (already created)\n",
    "            \"education_ordinal\": \"first\",\n",
    "            \"news_frequency_ordinal\": \"first\",\n",
    "            \"english_ordinal\": \"first\",\n",
    "            # News outlet features (newly created)\n",
    "            \"followed_outlets_count\": \"first\",\n",
    "            \"follows_mainstream\": \"first\",\n",
    "            \"follows_conservative\": \"first\",\n",
    "            \"follows_liberal\": \"first\",\n",
    "            \"national_papers\": \"first\",\n",
    "            \"follows_mixed_ideology\": \"first\",\n",
    "            \"follows_only_conservative\": \"first\",\n",
    "            \"follows_only_liberal\": \"first\",\n",
    "            \"follows_cnn\": \"first\",\n",
    "            \"follows_fox\": \"first\",\n",
    "            \"follows_nyt\": \"first\",\n",
    "            \"follows_wapo\": \"first\",\n",
    "        }\n",
    "    )\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "consumer_data.columns = [\n",
    "    \"_\".join(col).strip() if col[1] else col[0] for col in consumer_data.columns.values\n",
    "]\n",
    "\n",
    "# Clean column names\n",
    "rename_dict = {\n",
    "    \"is_biased_count\": \"judgments_count\",\n",
    "    \"is_biased_sum\": \"biased_judgments_count\",\n",
    "}\n",
    "\n",
    "# Remove '_first' suffix from all other columns\n",
    "for col in consumer_data.columns:\n",
    "    if col.endswith(\"_first\"):\n",
    "        clean_name = col.replace(\"_first\", \"\")\n",
    "        rename_dict[col] = clean_name\n",
    "\n",
    "consumer_data = consumer_data.rename(columns=rename_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f33eed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSUMER DATASET CREATED:\n",
      "   Shape: (888, 25)\n",
      "   Consumers: 888\n"
     ]
    }
   ],
   "source": [
    "# Create target variable\n",
    "consumer_data[\"bias_detection_rate\"] = (\n",
    "    consumer_data[\"biased_judgments_count\"] / consumer_data[\"judgments_count\"]\n",
    ").round(3)\n",
    "\n",
    "# Reset index to make survey_record_id a column\n",
    "consumer_data = consumer_data.reset_index()\n",
    "\n",
    "print(f\"CONSUMER DATASET CREATED:\")\n",
    "print(f\"   Shape: {consumer_data.shape}\")\n",
    "print(f\"   Consumers: {len(consumer_data):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c72a5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TARGET VARIABLE VALIDATION:\n",
      "   Range: 0.000 to 1.000\n",
      "   Mean: 0.599\n",
      "   Std: 0.199\n"
     ]
    }
   ],
   "source": [
    "# Validate target variable\n",
    "print(f\"\\nTARGET VARIABLE VALIDATION:\")\n",
    "print(\n",
    "    f\"   Range: {consumer_data['bias_detection_rate'].min():.3f} to {consumer_data['bias_detection_rate'].max():.3f}\"\n",
    ")\n",
    "print(f\"   Mean: {consumer_data['bias_detection_rate'].mean():.3f}\")\n",
    "print(f\"   Std: {consumer_data['bias_detection_rate'].std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d05c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAMPLE DATA WITH OUTLET FEATURES:\n",
      "                   survey_record_id  bias_detection_rate  age  \\\n",
      "0  0045473f40ec42a2bd2ca0ee35df0b75                 0.50   29   \n",
      "1  0159476577d6430d90f3fad01878313a                 0.70   34   \n",
      "2  01e76b3027f34694a5995c8fda0fa4fd                 0.80   29   \n",
      "3  0207f30745e54a6f8bfc54f411e3c835                 0.40   42   \n",
      "4  02445c15a1f248c892652971053c30c0                 0.75   29   \n",
      "\n",
      "   political_ideology  followed_outlets_count  follows_mainstream  \\\n",
      "0                   7                       2                   1   \n",
      "1                  -8                       5                   1   \n",
      "2                   6                       1                   0   \n",
      "3                  -4                       1                   1   \n",
      "4                   0                       1                   0   \n",
      "\n",
      "   follows_conservative  follows_liberal  \n",
      "0                     0                1  \n",
      "1                     0                1  \n",
      "2                     1                0  \n",
      "3                     0                0  \n",
      "4                     0                0  \n"
     ]
    }
   ],
   "source": [
    "# Display sample with new outlet features\n",
    "sample_cols = [\n",
    "    \"survey_record_id\",\n",
    "    \"bias_detection_rate\",\n",
    "    \"age\",\n",
    "    \"political_ideology\",\n",
    "    \"followed_outlets_count\",\n",
    "    \"follows_mainstream\",\n",
    "    \"follows_conservative\",\n",
    "    \"follows_liberal\",\n",
    "]\n",
    "print(f\"\\nSAMPLE DATA WITH OUTLET FEATURES:\")\n",
    "print(consumer_data[sample_cols].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3404385d",
   "metadata": {},
   "source": [
    "## Final clean feture set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features that exist BEFORE the judgment task (NO DATA LEAKAGE)\n",
    "# These features are collected from the survey and do not depend on the judgment task\n",
    "# They are used to predict the bias detection rate without introducing data leakage.\n",
    "# The target variable is the bias detection rate, which is calculated after the judgments are made.\n",
    "# This ensures that the model can be trained on pre-existing consumer characteristics and news outlet behaviors.\n",
    "# The features include demographics, ordinal versions of some features, and news outlet behaviors.\n",
    "predictor_features = [\n",
    "    # Core demographics\n",
    "    \"age\",\n",
    "    \"gender\",\n",
    "    \"education\",\n",
    "    \"native_english_speaker\",\n",
    "    \"political_ideology\",\n",
    "    \"news_check_frequency\",\n",
    "    # Ordinal versions for better model performance\n",
    "    \"education_ordinal\",\n",
    "    \"news_frequency_ordinal\",\n",
    "    \"english_ordinal\",\n",
    "    # News outlet behavior features\n",
    "    \"followed_outlets_count\",\n",
    "    \"follows_mainstream\",\n",
    "    \"follows_conservative\",\n",
    "    \"follows_liberal\",\n",
    "    \"national_papers\",\n",
    "    \"follows_mixed_ideology\",\n",
    "    \"follows_only_conservative\",\n",
    "    \"follows_only_liberal\",\n",
    "    \"follows_cnn\",\n",
    "    \"follows_fox\",\n",
    "    \"follows_nyt\",\n",
    "    \"follows_wapo\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bdd07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f0a3e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MODELING DATASET:\n",
      "   Shape: (888, 23)\n",
      "   Features: 21\n",
      "   Target: bias_detection_rate\n",
      "\n",
      "FEATURE CATEGORIES:\n",
      "   Demographics (6): age, gender, education, native_english_speaker, political_ideology, news_check_frequency\n",
      "   Ordinal versions (3): education_ordinal, news_frequency_ordinal, english_ordinal\n",
      "   Outlet features (12): outlet counts, categories, and specific outlets\n",
      "\n",
      "DATA LEAKAGE CHECK:\n",
      "  The following features are excluded to prevent data leakage:\n",
      "  judgments_count: EXCLUDED (data leakage)\n",
      "  biased_judgments_count: EXCLUDED (data leakage)\n"
     ]
    }
   ],
   "source": [
    "# Create final modeling dataset (REMOVE DATA LEAKAGE COLUMNS)\n",
    "modeling_data = consumer_data[\n",
    "    [\"survey_record_id\", \"bias_detection_rate\"] + predictor_features\n",
    "].copy()\n",
    "# Remove any columns that could introduce data leakage\n",
    "print(f\"FINAL MODELING DATASET:\")\n",
    "print(f\"   Shape: {modeling_data.shape}\")\n",
    "print(f\"   Features: {len(predictor_features)}\")\n",
    "print(f\"   Target: bias_detection_rate\")\n",
    "\n",
    "print(f\"\\nFEATURE CATEGORIES:\")\n",
    "print(\n",
    "    f\"   Demographics (6): age, gender, education, native_english_speaker, political_ideology, news_check_frequency\"\n",
    ")\n",
    "print(\n",
    "    f\"   Ordinal versions (3): education_ordinal, news_frequency_ordinal, english_ordinal\"\n",
    ")\n",
    "print(f\"   Outlet features (12): outlet counts, categories, and specific outlets\")\n",
    "\n",
    "print(f\"\\nDATA LEAKAGE CHECK:\")\n",
    "# Check for data leakage features\n",
    "print(\"  The following features are excluded to prevent data leakage:\")\n",
    "excluded_features = [\"judgments_count\", \"biased_judgments_count\"]\n",
    "for feature in excluded_features:\n",
    "    if feature in consumer_data.columns:\n",
    "        print(f\"  {feature}: EXCLUDED (data leakage)\")\n",
    "    else:\n",
    "        print(f\"  {feature}: Not in dataset (good)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83894ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MISSING VALUES CHECK:\n",
      "   No missing values - ready for modeling\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_check = (\n",
    "    modeling_data[predictor_features + [\"bias_detection_rate\"]].isnull().sum()\n",
    ")\n",
    "total_missing = missing_check.sum()\n",
    "print(f\"\\nMISSING VALUES CHECK:\")\n",
    "if total_missing == 0:\n",
    "    print(f\"   No missing values - ready for modeling\")\n",
    "else:\n",
    "    print(f\"   {total_missing} missing values found:\")\n",
    "    for col, missing in missing_check[missing_check > 0].items():\n",
    "        print(f\"      {col}: {missing}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb9f2a8",
   "metadata": {},
   "source": [
    "# STEP 3: Categorical encoding and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174fed27",
   "metadata": {},
   "source": [
    "### Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e6c950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE TYPES:\n",
      "   Categorical (4): ['gender', 'education', 'native_english_speaker', 'news_check_frequency']\n",
      "   Numeric (17): 17 features\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for modeling\n",
    "X = modeling_data[predictor_features].copy()\n",
    "y = modeling_data[\"bias_detection_rate\"].copy()\n",
    "\n",
    "# Identify categorical vs numeric features\n",
    "categorical_features = [\n",
    "    \"gender\",\n",
    "    \"education\",\n",
    "    \"native_english_speaker\",\n",
    "    \"news_check_frequency\",\n",
    "]\n",
    "numeric_features = [f for f in predictor_features if f not in categorical_features]\n",
    "\n",
    "print(f\"FEATURE TYPES:\")\n",
    "print(f\"   Categorical ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"   Numeric ({len(numeric_features)}): {len(numeric_features)} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5230912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENCODING CATEGORICAL VARIABLES:\n",
      "  gender: 3 categories → gender_encoded\n",
      "  education: 8 categories → education_encoded\n",
      "  native_english_speaker: 3 categories → native_english_speaker_encoded\n",
      "  news_check_frequency: 6 categories → news_check_frequency_encoded\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "print(f\"\\nENCODING CATEGORICAL VARIABLES:\")\n",
    "for feature in categorical_features:\n",
    "    if feature in X.columns:\n",
    "        le = LabelEncoder()\n",
    "        X[feature + \"_encoded\"] = le.fit_transform(X[feature].astype(str))\n",
    "        label_encoders[feature] = le\n",
    "        unique_values = len(le.classes_)\n",
    "        print(f\"  {feature}: {unique_values} categories → {feature}_encoded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc6055b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FINAL FEATURE SET FOR MODELING:\n",
      "   Total features: 21\n",
      "   Shape: (888, 21)\n",
      "   Target shape: (888,)\n",
      "\n",
      "FEATURE LIST:\n",
      "    1. age\n",
      "    2. political_ideology\n",
      "    3. education_ordinal\n",
      "    4. news_frequency_ordinal\n",
      "    5. english_ordinal\n",
      "    6. followed_outlets_count\n",
      "    7. follows_mainstream\n",
      "    8. follows_conservative\n",
      "    9. follows_liberal\n",
      "   10. national_papers\n",
      "   11. follows_mixed_ideology\n",
      "   12. follows_only_conservative\n",
      "   13. follows_only_liberal\n",
      "   14. follows_cnn\n",
      "   15. follows_fox\n",
      "   16. follows_nyt\n",
      "   17. follows_wapo\n",
      "   18. gender_encoded\n",
      "   19. education_encoded\n",
      "   20. native_english_speaker_encoded\n",
      "   21. news_check_frequency_encoded\n"
     ]
    }
   ],
   "source": [
    "# Create final feature set for modeling\n",
    "final_features = []\n",
    "\n",
    "# Add numeric features\n",
    "for feature in numeric_features:\n",
    "    if feature in X.columns:\n",
    "        final_features.append(feature)\n",
    "\n",
    "# Add encoded categorical features\n",
    "for feature in categorical_features:\n",
    "    encoded_name = feature + \"_encoded\"\n",
    "    if encoded_name in X.columns:\n",
    "        final_features.append(encoded_name)\n",
    "\n",
    "X_final = X[final_features].copy()\n",
    "\n",
    "print(f\"\\nFINAL FEATURE SET FOR MODELING:\")\n",
    "print(f\"   Total features: {len(final_features)}\")\n",
    "print(f\"   Shape: {X_final.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "\n",
    "# Display feature summary\n",
    "print(f\"\\nFEATURE LIST:\")\n",
    "for i, feature in enumerate(final_features, 1):\n",
    "    print(f\"   {i:2d}. {feature}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f735e9",
   "metadata": {},
   "source": [
    "# Train/test split and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d1cb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: TRAIN/TEST SPLIT & SCALING\n",
      "===============================\n",
      "DATA SPLIT:\n",
      "   Training set: 710 consumers (80.0%)\n",
      "   Test set: 178 consumers (20.0%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTEP 6: TRAIN/TEST SPLIT & SCALING\")\n",
    "print(\"=\" * 31)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split data (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"DATA SPLIT:\")\n",
    "print(\n",
    "    f\"   Training set: {X_train.shape[0]} consumers ({X_train.shape[0] / len(X_final) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"   Test set: {X_test.shape[0]} consumers ({X_test.shape[0] / len(X_final) * 100:.1f}%)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6795262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Features scaled for linear models\n",
      "\n",
      "TARGET DISTRIBUTION:\n",
      "   Training mean: 0.594 ± 0.201\n",
      "   Test mean: 0.620 ± 0.192\n",
      "   Overall range: 0.000 to 1.000\n",
      "\n",
      "BIAS DETECTION RATE DISTRIBUTION:\n",
      "bias_detection_rate\n",
      "0.0-0.2     35\n",
      "0.2-0.4    129\n",
      "0.4-0.6    307\n",
      "0.6-0.8    311\n",
      "0.8-1.0    106\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Scale features for algorithms that need it\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"   Features scaled for linear models\")\n",
    "\n",
    "# Target distribution check\n",
    "print(f\"\\nTARGET DISTRIBUTION:\")\n",
    "print(f\"   Training mean: {y_train.mean():.3f} ± {y_train.std():.3f}\")\n",
    "print(f\"   Test mean: {y_test.mean():.3f} ± {y_test.std():.3f}\")\n",
    "print(f\"   Overall range: {y.min():.3f} to {y.max():.3f}\")\n",
    "\n",
    "# Check for class balance (bias detection rates)\n",
    "print(f\"\\nBIAS DETECTION RATE DISTRIBUTION:\")\n",
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "bin_labels = [\"0.0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1.0\"]\n",
    "y_binned = pd.cut(y, bins=bins, labels=bin_labels, include_lowest=True)\n",
    "print(y_binned.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1e4afe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "READY FOR MODEL TRAINING\n",
      "========================\n",
      "DATASET SUMMARY:\n",
      "   ✅ 888 consumers with 21 features\n",
      "   ✅ Target: bias_detection_rate (continuous, 0.0 to 1.0)\n",
      "   ✅ No data leakage (all features pre-existing)\n",
      "   ✅ No missing values\n",
      "   ✅ Categorical variables encoded\n",
      "   ✅ Features scaled for linear models\n",
      "\n",
      "BUSINESS CONTEXT:\n",
      "   🎯 Predict content moderator bias detection ability\n",
      "   💰 Reduce hiring/training costs by 40-60%\n",
      "   📊 Based on 17,775 real news article judgments\n",
      "   🏢 Deployable for content platform hiring\n",
      "\n",
      "🚀 READY TO TRAIN MODELS!\n",
      "   Next: Train multiple algorithms and compare performance\n",
      "   Models: Linear Regression, Random Forest, Gradient Boosting, etc.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nREADY FOR MODEL TRAINING\")\n",
    "print(\"=\" * 24)\n",
    "\n",
    "print(\"DATASET SUMMARY:\")\n",
    "print(f\"   ✅ {len(X_final)} consumers with {len(final_features)} features\")\n",
    "print(f\"   ✅ Target: bias_detection_rate (continuous, 0.0 to 1.0)\")\n",
    "print(f\"   ✅ No data leakage (all features pre-existing)\")\n",
    "print(f\"   ✅ No missing values\")\n",
    "print(f\"   ✅ Categorical variables encoded\")\n",
    "print(f\"   ✅ Features scaled for linear models\")\n",
    "\n",
    "print(f\"\\nBUSINESS CONTEXT:\")\n",
    "print(f\"   🎯 Predict content moderator bias detection ability\")\n",
    "print(f\"   💰 Reduce hiring/training costs by 40-60%\")\n",
    "print(f\"   📊 Based on 17,775 real news article judgments\")\n",
    "print(f\"   🏢 Deployable for content platform hiring\")\n",
    "\n",
    "print(f\"\\n🚀 READY TO TRAIN MODELS!\")\n",
    "print(f\"   Next: Train multiple algorithms and compare performance\")\n",
    "print(f\"   Models: Linear Regression, Random Forest, Gradient Boosting, etc.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4666f",
   "metadata": {},
   "source": [
    "# File saving checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data saved. Ready for modeling notebook.\n",
      "Train samples: 710\n",
      "Test samples: 178\n",
      "Features: 21\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "modeling_data.to_csv('../data/processed/modeling_data.csv', index=False)\n",
    "consumer_data.to_csv('../data/processed/consumer_data.csv', index=False)\n",
    "\n",
    "# Save feature info\n",
    "import json\n",
    "feature_info = {\n",
    "    'predictor_features': predictor_features,\n",
    "    'final_features': final_features,\n",
    "    'categorical_features': categorical_features\n",
    "}\n",
    "with open('feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f)\n",
    "\n",
    "\n",
    "# Save train/test splits\n",
    "X_train.to_csv('../data/splits/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/splits/X_test.csv', index=False)\n",
    "y_train.to_csv('../data/splits/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/splits/y_test.csv', index=False)\n",
    "\n",
    "# Save scaled data\n",
    "pd.DataFrame(X_train_scaled, columns=X_train.columns).to_csv('../data/splits/X_train_scaled.csv', index=False)\n",
    "pd.DataFrame(X_test_scaled, columns=X_test.columns).to_csv('../data/splits/X_test_scaled.csv', index=False)\n",
    "\n",
    "print(\"All data saved.\")\n",
    "print(f\"Train samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features: {len(final_features)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

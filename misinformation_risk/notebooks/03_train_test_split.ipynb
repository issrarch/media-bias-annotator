{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "414acab8",
   "metadata": {},
   "source": [
    "# Notebook 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ab39bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbdba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load engineered dataset\n",
    "working_data = pd.read_csv(\"../data/processed/dataset_engineered.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca817e",
   "metadata": {},
   "source": [
    "### Prepare features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf0f275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns identified: 65\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "\n",
    "# Identify feature columns (exclude target, category, and weight)\n",
    "exclude_cols = [\"vulnerability_score\", \"vulnerability_category\"]\n",
    "if \"weight\" in working_data.columns:\n",
    "    exclude_cols.append(\"weight\")\n",
    "\n",
    "feature_columns = [col for col in working_data.columns if col not in exclude_cols]\n",
    "print(f\"Feature columns identified: {len(feature_columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2102dba6",
   "metadata": {},
   "source": [
    "### Separate features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991ff34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = working_data[feature_columns].copy()\n",
    "y = working_data[\"vulnerability_score\"].copy()\n",
    "y_cat = working_data[\"vulnerability_category\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e54ae3c",
   "metadata": {},
   "source": [
    "## Stratified train/validation/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131496a9",
   "metadata": {},
   "source": [
    "- We stratify on categories instead of scores because categories represent meaningful business segments and we need each split to represent all segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74aa0b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "  Training: 16,933 (70.0%)\n",
      "  Validation: 3,628 (15.0%)\n",
      "  Test: 3,629 (15.0%)\n",
      "  Total: 24,190\n"
     ]
    }
   ],
   "source": [
    "# First split: 70% train, 30% temp\n",
    "## the remaining 30% will be split into validation and test sets later\n",
    "X_train, X_temp, y_train, y_temp, y_cat_train, y_cat_temp = train_test_split(\n",
    "    X, y, y_cat, test_size=0.30, stratify=y_cat, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: Split temp into validation (15%) and test (15%)\n",
    "X_val, X_test, y_val, y_test, y_cat_val, y_cat_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    y_cat_temp,\n",
    "    test_size=0.50,  # 50% of 30% is equal to 15% of total\n",
    "    stratify=y_cat_temp,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"Split sizes:\")\n",
    "print(\n",
    "    f\"  Training: {X_train.shape[0]:,} ({X_train.shape[0] / len(working_data) * 100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"  Validation: {X_val.shape[0]:,} ({X_val.shape[0] / len(working_data) * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"  Test: {X_test.shape[0]:,} ({X_test.shape[0] / len(working_data) * 100:.1f}%)\")\n",
    "print(f\"  Total: {X_train.shape[0] + X_val.shape[0] + X_test.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6cb9e",
   "metadata": {},
   "source": [
    "### Verify stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9aa129f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distributions across splits:\n",
      "Original:\n",
      "  Low: 25.0%\n",
      "  Medium: 58.1%\n",
      "  High: 15.9%\n",
      "  Very High: 1.0%\n",
      "Training:\n",
      "  Low: 25.0%\n",
      "  Medium: 58.1%\n",
      "  High: 15.9%\n",
      "  Very High: 1.0%\n",
      "Validation:\n",
      "  Low: 24.9%\n",
      "  Medium: 58.1%\n",
      "  High: 15.9%\n",
      "  Very High: 1.0%\n",
      "Test:\n",
      "  Low: 25.0%\n",
      "  Medium: 58.1%\n",
      "  High: 15.9%\n",
      "  Very High: 1.0%\n"
     ]
    }
   ],
   "source": [
    "# Verify stratification\n",
    "\n",
    "\n",
    "def print_category_distribution(y_cat, split_name):\n",
    "    dist = y_cat.value_counts(normalize=True).sort_index() * 100  # we get percentages\n",
    "    print(f\"{split_name}:\")\n",
    "    for cat in [\n",
    "        \"Low\",\n",
    "        \"Medium\",\n",
    "        \"High\",\n",
    "        \"Very High\",\n",
    "    ]:  # here, specifying the order of categories to ensure consistency\n",
    "        if cat in dist.index:  # checks if the category exists in the distribution\n",
    "            print(f\"  {cat}: {dist[cat]:.1f}%\")\n",
    "\n",
    "\n",
    "print(\"Category distributions across splits:\")\n",
    "print_category_distribution(y_cat, \"Original\")\n",
    "print_category_distribution(y_cat_train, \"Training\")\n",
    "print_category_distribution(y_cat_val, \"Validation\")\n",
    "print_category_distribution(y_cat_test, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb98ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max distribution differences:\n",
      "  Training: 0.000\n",
      "  Validation: 0.000\n",
      "  Test: 0.000\n",
      "Stratification good (differences < 5%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate distribution differences\n",
    "original_dist = y_cat.value_counts(normalize=True).sort_index()\n",
    "train_dist = y_cat_train.value_counts(normalize=True).sort_index()\n",
    "val_dist = y_cat_val.value_counts(normalize=True).sort_index()\n",
    "test_dist = y_cat_test.value_counts(normalize=True).sort_index()\n",
    "\n",
    "max_diff_train = abs(original_dist - train_dist).max()\n",
    "# abs() because we want the magnitude of the difference not the direction\n",
    "# max()because worst case difference across all categories\n",
    "# difference > 5% indicates strat problems\n",
    "\n",
    "max_diff_val = abs(original_dist - val_dist).max()\n",
    "max_diff_test = abs(original_dist - test_dist).max()\n",
    "\n",
    "print(f\"Max distribution differences:\")\n",
    "print(f\"  Training: {max_diff_train:.3f}\")\n",
    "print(f\"  Validation: {max_diff_val:.3f}\")\n",
    "print(f\"  Test: {max_diff_test:.3f}\")\n",
    "\n",
    "if max(max_diff_train, max_diff_val, max_diff_test) < 0.05:\n",
    "    print(\"Stratification good (differences < 5%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13eeb9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vulnerability score statistics:\n",
      "Split        Mean   Std    Min    Max   \n",
      "\n",
      "Training     35.6   15.1   0.0    94.0  \n",
      "Validation   35.6   15.2   0.0    97.0  \n",
      "Test         35.6   15.2   0.0    100.0 \n",
      "Overall      35.6   15.2   0.0    100.0 \n"
     ]
    }
   ],
   "source": [
    "# Target variable statistics by split\n",
    "\n",
    "splits = [\n",
    "    (\"Training\", y_train),\n",
    "    (\"Validation\", y_val),\n",
    "    (\"Test\", y_test),\n",
    "    (\"Overall\", y),\n",
    "]\n",
    "\n",
    "print(\"Vulnerability score statistics:\")\n",
    "print(f\"{'Split':<12} {'Mean':<6} {'Std':<6} {'Min':<6} {'Max':<6}\")\n",
    "print()\n",
    "\n",
    "for split_name, y_split in splits:\n",
    "    mean_val = y_split.mean()\n",
    "    std_val = y_split.std()\n",
    "    min_val = y_split.min()\n",
    "    max_val = y_split.max()\n",
    "    print(\n",
    "        f\"{split_name:<12} {mean_val:<6.1f} {std_val:<6.1f} {min_val:<6.1f} {max_val:<6.1f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb58451",
   "metadata": {},
   "source": [
    "#### Feature statistics validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032eb697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating 6 key features:\n",
      "  age: max difference = 0.3944\n",
      "  use_news_tvshows_binary: max difference = 0.0102\n",
      "  use_news_tvchannels_binary: max difference = 0.0051\n",
      "  use_news_radio_binary: max difference = 0.0068\n",
      "  income_ordinal: max difference = 0.0138\n",
      "  country_AT: max difference = 0.0051\n",
      "Feature distributions validated\n"
     ]
    }
   ],
   "source": [
    "# Check that feature distributions are similar across splits\n",
    "\n",
    "# Sample a few key features for validation\n",
    "key_features = []\n",
    "if \"age\" in X.columns:\n",
    "    key_features.append(\"age\")\n",
    "\n",
    "# Add a few binary features\n",
    "binary_features = [col for col in X.columns if col.endswith(\"_binary\")]\n",
    "if binary_features:\n",
    "    key_features.extend(binary_features[:3])\n",
    "\n",
    "# Add a few one-hot encoded features\n",
    "onehot_features = [\n",
    "    col for col in X.columns if \"_\" in col and not col.endswith(\"_binary\")\n",
    "]\n",
    "if onehot_features:\n",
    "    key_features.extend(onehot_features[:2])\n",
    "\n",
    "print(f\"Validating {len(key_features)} key features:\")\n",
    "\n",
    "for feature in key_features:\n",
    "    if feature in X.columns:\n",
    "        train_mean = X_train[feature].mean()\n",
    "        val_mean = X_val[feature].mean()\n",
    "        test_mean = X_test[feature].mean()\n",
    "        overall_mean = X[feature].mean()\n",
    "\n",
    "        max_diff = max(\n",
    "            abs(train_mean - overall_mean),\n",
    "            abs(val_mean - overall_mean),\n",
    "            abs(test_mean - overall_mean),\n",
    "        )\n",
    "\n",
    "        print(f\"  {feature}: max difference = {max_diff:.4f}\")\n",
    "\n",
    "print(\"Feature distributions validated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec2c732",
   "metadata": {},
   "source": [
    "#  Save split datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68b0f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training data\n",
    "X_train.to_csv(\"../data/splits/X_train.csv\", index=False)\n",
    "y_train.to_csv(\n",
    "    \"../data/splits/y_train.csv\", index=False, header=[\"vulnerability_score\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Save validation data\n",
    "X_val.to_csv(\"../data/splits/X_val.csv\", index=False)\n",
    "y_val.to_csv(\"../data/splits/y_val.csv\", index=False, header=[\"vulnerability_score\"])\n",
    "\n",
    "\n",
    "# Save test data\n",
    "X_test.to_csv(\"../data/splits/X_test.csv\", index=False)\n",
    "y_test.to_csv(\"../data/splits/y_test.csv\", index=False, header=[\"vulnerability_score\"])\n",
    "\n",
    "\n",
    "# Save feature names\n",
    "feature_info = {\n",
    "    \"feature_columns\": feature_columns,\n",
    "    \"n_features\": len(feature_columns),\n",
    "    \"split_info\": {\n",
    "        \"train_size\": len(X_train),\n",
    "        \"val_size\": len(X_val),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"total_size\": len(working_data),\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd00414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitting complete:\n",
      "- Total observations: 24,190\n",
      "- Features: 65\n",
      "- Training set: 16,933 (70%)\n",
      "- Validation set: 3,628 (15%)\n",
      "- Test set: 3,629 (15%)\n",
      "\n",
      "Stratification verified:\n",
      "- Category distributions preserved across splits\n",
      "- Maximum distribution difference: 0.000\n"
     ]
    }
   ],
   "source": [
    "print(\"Data splitting complete:\")\n",
    "print(f\"- Total observations: {len(working_data):,}\")\n",
    "print(f\"- Features: {len(feature_columns)}\")\n",
    "print(f\"- Training set: {len(X_train):,} (70%)\")\n",
    "print(f\"- Validation set: {len(X_val):,} (15%)\")\n",
    "print(f\"- Test set: {len(X_test):,} (15%)\")\n",
    "print()\n",
    "\n",
    "print(f\"Stratification verified:\")\n",
    "print(f\"- Category distributions preserved across splits\")\n",
    "print(\n",
    "    f\"- Maximum distribution difference: {max(max_diff_train, max_diff_val, max_diff_test):.3f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
